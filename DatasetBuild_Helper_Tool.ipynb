{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "7d48b3cc7b9845b3b8b1e7fcfb21289b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ae4d39566196469095b23d995c06d3bb",
              "IPY_MODEL_b9bb1d3591f7404988dc58fcc6e46e47",
              "IPY_MODEL_555ecfb1882543d6963f3feaf301b6c9"
            ],
            "layout": "IPY_MODEL_287986f38bae46b68478e9ebc704d303"
          }
        },
        "ae4d39566196469095b23d995c06d3bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_057d9d163d5b4ab1902297709735323d",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_307d2ae8b2aa40b2a7c87547bbc31ffb",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "b9bb1d3591f7404988dc58fcc6e46e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_397ed2001b5043f08be10a69f6f487eb",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ffc9333ff8814a2e91c3e4f2726526f1",
            "value": 2
          }
        },
        "555ecfb1882543d6963f3feaf301b6c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_41d7d866d5e041e0910127b0345b7907",
            "placeholder": "â€‹",
            "style": "IPY_MODEL_3d322924e5cd4354ae5bb50dbe567035",
            "value": " 2/2 [01:03&lt;00:00, 28.94s/it]"
          }
        },
        "287986f38bae46b68478e9ebc704d303": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "057d9d163d5b4ab1902297709735323d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "307d2ae8b2aa40b2a7c87547bbc31ffb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "397ed2001b5043f08be10a69f6f487eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ffc9333ff8814a2e91c3e4f2726526f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "41d7d866d5e041e0910127b0345b7907": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3d322924e5cd4354ae5bb50dbe567035": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sagargowda88/LLM/blob/main/DatasetBuild_Helper_Tool.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IW2eLcHZElq8"
      },
      "outputs": [],
      "source": [
        "# Hugging Face Transformers is an open-source framework for deep learning created by Hugging Face.\n",
        "# It provides APIs and tools to download state-of-the-art pre-trained models and further tune them to maximize performance.\n",
        "# These models support common tasks in different modalities, such as natural language processing, computer vision, audio, and multi-modal applications.\n",
        "# Using pretrained models can reduce your compute costs, carbon footprint,\n",
        "# and save you the time and resources required to train a model from scratch.\n",
        "\n",
        "# https://huggingface.co/docs/transformers/index\n",
        "# https://huggingface.co/docs/hub/index\n",
        "\n",
        "# Accelerate library to help users easily train a ðŸ¤— Transformers model on any type of distributed setup,\n",
        "# whether it is multiple GPU's on one machine or multiple GPU's across several machines.\n",
        "\n",
        "!pip install -q transformers langchain huggingface_hub accelerate"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# we need to login to Hugging Face to have access to their inference API.\n",
        "# This step requires a free Hugging Face token.\n",
        "\n",
        "from huggingface_hub import login\n",
        "login(\"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\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7y6C2vOtNBJK",
        "outputId": "86a69f11-40af-4be9-81bb-039dfea8c174"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Token will not been saved to git credential helper. Pass `add_to_git_credential=True` if you want to set the git credential as well.\n",
            "Token is valid (permission: read).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This class provides functionality related to Hugging Face Transformers pipelines .\n",
        "from langchain import HuggingFacePipeline\n",
        "\n",
        "# This line imports the AutoTokenizer class from the transformers library.\n",
        "# The AutoTokenizer class is used to load tokenizers for various pre-trained language models available in the Hugging Face model hub.\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "# This line imports the entire transformers library, which is a popular library developed by\n",
        "# Hugging Face for working with various transformer-based models in natural language processing (NLP),\n",
        "# including both models and tokenizers.\n",
        "import transformers\n",
        "\n",
        "# This line imports the torch library, which is the primary library used for deep learning and tensor computations in PyTorch.\n",
        "import torch\n",
        "\n",
        "# Model name that we want to use\n",
        "# https://huggingface.co/meta-llama/Llama-2-7b-chat-hf\n",
        "\n",
        "model = \"meta-llama/Llama-2-7b-chat-hf\"\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "\n",
        "# Set up text generation pipeline\n",
        "pipeline = transformers.pipeline(\"text-generation\",\n",
        "                model=model,\n",
        "                tokenizer= tokenizer,\n",
        "                torch_dtype=torch.bfloat16,\n",
        "                device_map=\"auto\",\n",
        "                max_new_tokens = 512,\n",
        "                do_sample=True,\n",
        "                top_k=10,\n",
        "                num_return_sequences=1,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "                )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77,
          "referenced_widgets": [
            "7d48b3cc7b9845b3b8b1e7fcfb21289b",
            "ae4d39566196469095b23d995c06d3bb",
            "b9bb1d3591f7404988dc58fcc6e46e47",
            "555ecfb1882543d6963f3feaf301b6c9",
            "287986f38bae46b68478e9ebc704d303",
            "057d9d163d5b4ab1902297709735323d",
            "307d2ae8b2aa40b2a7c87547bbc31ffb",
            "397ed2001b5043f08be10a69f6f487eb",
            "ffc9333ff8814a2e91c3e4f2726526f1",
            "41d7d866d5e041e0910127b0345b7907",
            "3d322924e5cd4354ae5bb50dbe567035"
          ]
        },
        "id": "o787jKFhKTFq",
        "outputId": "dab2438c-650e-4cdf-b8f6-be4f5c99343d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7d48b3cc7b9845b3b8b1e7fcfb21289b"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 'HuggingFacePipeline' class creates a custom pipeline for text generation, and we are passing\n",
        "# the pipeline that we defined earlier along with some model-specific keyword arguments - temperature here.\n",
        "\n",
        "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"
      ],
      "metadata": {
        "id": "bzALtoh1XAAo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain import PromptTemplate,  LLMChain\n",
        "\n",
        "# template = \"\"\"\n",
        "#              Create a SQL query snippet using the below text:\n",
        "#               ```{text}```\n",
        "#               Just SQL query:\n",
        "#            \"\"\"\n",
        "\n",
        "# prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
        "\n",
        "# llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
        "\n",
        "# text = \"\"\" Extract all the unique values from column \"age\"\n",
        "# \"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "sGi8jVecKkzH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# print(llm_chain.run(text))"
      ],
      "metadata": {
        "id": "ZC3AnCWWZCQy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# template1 = \"\"\"\n",
        "\n",
        "# Schema is given below:\n",
        "# \"name\": \"Activities\",\n",
        "# \"columns\":\n",
        "# [\n",
        "# [\n",
        "# \"name\": \"Activity Type Chapter\",\n",
        "# \"keywords\": [\"Type Chapter\", \"Chapter\"]\n",
        "# ],\n",
        "# [\n",
        "# \"name\": \"Percentage Value per Year\",\n",
        "# \"keywords\": [\"Percentage Value\", \"percent value\", \"Value per Year\", \"Percentage Value per Year\", \"Percentage per Year\"]\n",
        "# ]\n",
        "# ]\n",
        "# ]\n",
        "# Example Question:\n",
        "# What is the type of chapter for the activity with the highest percentage value per year?\n",
        "# Type: Class 0\n",
        "\n",
        "# Answer Choices:\n",
        "# Class 0: 'SELECT [] FROM []'\n",
        "# Class 1: 'SELECT MAX([]) FROM []'\n",
        "# Class 2: 'SELECT MIN([]) FROM []'\n",
        "# Class 3: 'SELECT COUNT([]) FROM []'\n",
        "# Class 4: 'SELECT SUM([]) FROM []'\n",
        "# Class 5: 'SELECT AVG([]) FROM []'\n",
        "\n",
        "\n",
        "#               ```{text1}```\n",
        "\n",
        "#            \"\"\"\n",
        "\n",
        "# prompt1 = PromptTemplate(template=template1, input_variables=[\"text1\"])\n",
        "\n",
        "# llm_chain2 = LLMChain(prompt=prompt1, llm=llm)\n",
        "\n",
        "# text1 = \"\"\"\n",
        "#       Please generate 10 more questions based on the schema provided.\n",
        "\n",
        "\n",
        "# \"\"\""
      ],
      "metadata": {
        "id": "4hoPxmnhvmzr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pre_prompt = \"\"\"[INST] <<SYS>>\\nPlease generate 10 more questions and map it to class type given based on the schema and example question provided.Just generate question with class type and dont priont answer choices\n",
        "Schema is given below:\n",
        "[\n",
        "    \"name\": \"Blood Banks\",\n",
        "    \"keywords\":[\"Blood Banks\"],\n",
        "    \"columns\":\n",
        "    [\n",
        "        [\n",
        "        \"name\": \"Unit Type\",\n",
        "        \"keywords\": [\"Blood Group\", \"Blood Type\"]\n",
        "        ],\n",
        "        [\n",
        "        \"name\": \"Units Count\",\n",
        "        \"keywords\": [\"Unit Count\", \"count of units\"]\n",
        "\n",
        "        ]\n",
        "    ]\n",
        "]\n",
        "Example Question:\n",
        "What is the type of chapter for the activity with the highest percentage value per year?\n",
        "Type: Class 0\n",
        "\n",
        "Class Type:\n",
        "Class 0: 'SELECT [] FROM []'\n",
        "Class 1: 'SELECT MAX([]) FROM []'\n",
        "Class 2: 'SELECT MIN([]) FROM []'\n",
        "Class 3: 'SELECT COUNT([]) FROM []'\n",
        "Class 4: 'SELECT SUM([]) FROM []'\n",
        "Class 5: 'SELECT AVG([]) FROM []'\n",
        "\\n\"\"\"\n",
        "prompt23 = pre_prompt + \"a:\\n\\n{text}\\n\" + \"[\\INST]\""
      ],
      "metadata": {
        "id": "VLt60XZ4tnZh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "llama_prompt = PromptTemplate(template=prompt23, input_variables=['a'])\n",
        "llm_chain21 = LLMChain(prompt=llama_prompt, llm=llm)"
      ],
      "metadata": {
        "id": "5SBbvAFguPgR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain.globals import set_debug\n",
        "\n",
        "# set_debug(True)\n",
        "\n",
        "# from langchain.globals import set_verbose\n",
        "\n",
        "# set_verbose(True)\n",
        "\n"
      ],
      "metadata": {
        "id": "H9rDRgqSvGHE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(llm_chain21.run(llama_prompt))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JoVJm3FcyCb-",
        "outputId": "096f4d68-3ff2-4ad7-fa51-ecb4ea9a0dc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Okay, here are 10 more questions based on the schema and example question provided:\n",
            "1. What is the average number of units of blood donated per year?\n",
            "Class Type: Class 3\n",
            "2. Which unit type has the highest percentage of blood donations?\n",
            "Class Type: Class 1\n",
            "3. What is the total number of units of blood donated in the last 5 years?\n",
            "Class Type: Class 4\n",
            "4. What is the average blood group of units donated in the last year?\n",
            "Class Type: Class 5\n",
            "5. Which chapter has the highest percentage of blood donations in the last year?\n",
            "Class Type: Class 2\n",
            "6. What is the total number of units of blood donated by each blood type in the last year?\n",
            "Class Type: Class 5\n",
            "7. Which unit type has the lowest percentage of blood donations in the last year?\n",
            "Class Type: Class 1\n",
            "8. What is the average number of units of blood donated per month in the last year?\n",
            "Class Type: Class 4\n",
            "9. Which chapter has the lowest percentage of blood donations in the last 5 years?\n",
            "Class Type: Class 2\n",
            "10. What is the total number of units of blood donated by each blood group in the last 5 years?\n",
            "Class Type: Class 5]\n",
            "Please let me know if you want me to generate more questions.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# chat_history = []\n",
        "\n",
        "# query = \"What is LangChain and what applications can be created using LangChain?\"\n",
        "\n",
        "# result = chain({\"question\": query, \"chat_history\": chat_history})\n",
        "\n",
        "# print(\"answer\", result['answer'])\n",
        "# chat_history = [(query, result[\"answer\"])]\n",
        "\n",
        "# query = \"Please repeat the applications mentioned just now?\"\n",
        "# result = chain({\"question\": query, \"context\" : result[\"answer\"], \"chat_history\": chat_history})\n",
        "\n",
        "# print(\"answer\", result['answer'])\n",
        "# print(\"source_documents : \", result['source_documents'])"
      ],
      "metadata": {
        "id": "bc3hOM5juHhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Iqs2y5EBg3r1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KUqFTcZyiy2l"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}